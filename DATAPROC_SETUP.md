# Dataproc å¿«é€Ÿè®¾ç½®æŒ‡å—

## ğŸ¯ ç›®æ ‡

åœ¨ Google Cloud Dataproc ä¸Šè¿è¡Œ Spark ä½œä¸šï¼Œå¤„ç†æ•´åˆåçš„æ•°æ®ã€‚

---

## ğŸ“‹ å‰ç½®è¦æ±‚

1. **Google Cloud Platform è´¦æˆ·**
2. **gcloud CLI å·²å®‰è£…** - https://cloud.google.com/sdk/docs/install
3. **å·²å¯ç”¨çš„ API:**
   - Dataproc API
   - Cloud Storage API
   - Compute Engine API

---

## ğŸš€ å¿«é€Ÿå¼€å§‹ï¼ˆ5 æ­¥ï¼‰

### **Step 1: åˆ›å»º GCS Bucket**

```bash
# è®¾ç½®å˜é‡
export PROJECT_ID="your-project-id"
export BUCKET_NAME="your-bucket-name"
export REGION="us-east1"

# åˆ›å»º bucket
gsutil mb -p $PROJECT_ID -l $REGION gs://$BUCKET_NAME/

# éªŒè¯
gsutil ls gs://$BUCKET_NAME/
```

---

### **Step 2: åˆ›å»º Dataproc é›†ç¾¤**

#### **é€‰é¡¹ A: æ ‡å‡†é›†ç¾¤ï¼ˆæ¨èç”¨äºå¼€å‘ï¼‰**
```bash
gcloud dataproc clusters create ticketmaster-cluster \
  --project=$PROJECT_ID \
  --region=$REGION \
  --zone=${REGION}-b \
  --master-machine-type=n1-standard-4 \
  --master-boot-disk-size=100 \
  --num-workers=2 \
  --worker-machine-type=n1-standard-4 \
  --worker-boot-disk-size=100 \
  --image-version=2.1-debian11 \
  --optional-components=JUPYTER \
  --enable-component-gateway
```

**æˆæœ¬é¢„ä¼°:** ~$0.50-1.00/å°æ—¶

#### **é€‰é¡¹ B: æœ€å°é›†ç¾¤ï¼ˆèŠ‚çœæˆæœ¬ï¼‰**
```bash
gcloud dataproc clusters create ticketmaster-cluster-mini \
  --project=$PROJECT_ID \
  --region=$REGION \
  --zone=${REGION}-b \
  --single-node \
  --master-machine-type=n1-standard-2 \
  --master-boot-disk-size=50 \
  --image-version=2.1-debian11
```

**æˆæœ¬é¢„ä¼°:** ~$0.15-0.30/å°æ—¶

#### **é€‰é¡¹ C: ä¸´æ—¶é›†ç¾¤ï¼ˆæœ€çœé’±ï¼‰**
ä½œä¸šå®Œæˆåè‡ªåŠ¨åˆ é™¤ï¼š
```bash
# åœ¨æäº¤ä½œä¸šæ—¶åŠ ä¸Š --max-idle å‚æ•°
# è§ Step 4
```

---

### **Step 3: é…ç½®é¡¹ç›®**

ç¼–è¾‘ `dataproc_config.json`:
```json
{
  "project_id": "your-actual-project-id",
  "region": "us-east1",
  "cluster_name": "ticketmaster-cluster",
  "bucket_name": "your-actual-bucket-name",
  "data_path": "ticketmaster_data",
  "output_path": "ticketmaster_output"
}
```

---

### **Step 4: è¿è¡Œå¿«é€Ÿå¼€å§‹è„šæœ¬**

```bash
# Dataproc æ¨¡å¼
python quickstart_integration.py --mode dataproc
```

**è„šæœ¬ä¼šè‡ªåŠ¨ï¼š**
1. âœ… æœ¬åœ°æ•´åˆæ•°æ®
2. âœ… ä¸Šä¼ åˆ° GCS
3. âœ… æäº¤ ETL ä½œä¸š
4. âœ… æäº¤åˆ†æä½œä¸šï¼ˆå¯é€‰ï¼‰
5. âœ… æäº¤ ML ä½œä¸šï¼ˆå¯é€‰ï¼‰

---

### **Step 5: æŸ¥çœ‹ç»“æœ**

```bash
# æŸ¥çœ‹è¾“å‡ºæ–‡ä»¶
gsutil ls -r gs://$BUCKET_NAME/ticketmaster_output/

# ä¸‹è½½ç»“æœ
gsutil cp -r gs://$BUCKET_NAME/ticketmaster_output/ ./output/

# æŸ¥çœ‹ Parquet æ–‡ä»¶
python view_parquet.py output/ticketmaster_output/enriched_parquet/
```

---

## ğŸ› ï¸ æ‰‹åŠ¨æäº¤ä½œä¸šï¼ˆé«˜çº§ï¼‰

### **ETL ä½œä¸š**
```bash
gcloud dataproc jobs submit pyspark spark_etl_enriched.py \
  --cluster=ticketmaster-cluster \
  --region=us-east1 \
  --project=$PROJECT_ID \
  -- --input gs://$BUCKET_NAME/ticketmaster_data/enriched_events.csv \
     --output gs://$BUCKET_NAME/ticketmaster_output/enriched_parquet
```

### **åˆ†æä½œä¸š**
```bash
gcloud dataproc jobs submit pyspark spark_analysis_events.py \
  --cluster=ticketmaster-cluster \
  --region=us-east1 \
  --project=$PROJECT_ID \
  -- --input gs://$BUCKET_NAME/ticketmaster_output/enriched_parquet \
     --output gs://$BUCKET_NAME/ticketmaster_output/analytics
```

### **ML ä½œä¸šï¼ˆç¥¨ä»·é¢„æµ‹ï¼‰**
```bash
gcloud dataproc jobs submit pyspark spark_ml_price_prediction.py \
  --cluster=ticketmaster-cluster \
  --region=us-east1 \
  --project=$PROJECT_ID \
  -- --input gs://$BUCKET_NAME/ticketmaster_output/enriched_parquet \
     --metrics-output gs://$BUCKET_NAME/ticketmaster_output/ml/metrics \
     --model-output gs://$BUCKET_NAME/ticketmaster_output/ml/models/price_predictor \
     --model-type rf
```

---

## ğŸ’° æˆæœ¬ä¼˜åŒ–

### **1. ä½¿ç”¨ä¸´æ—¶é›†ç¾¤**
ä½œä¸šå®Œæˆåè‡ªåŠ¨åˆ é™¤ï¼š
```bash
gcloud dataproc jobs submit pyspark spark_etl_enriched.py \
  --cluster=ticketmaster-cluster-temp \
  --region=us-east1 \
  --project=$PROJECT_ID \
  --max-idle=10m \
  -- --input gs://$BUCKET_NAME/...
```

### **2. ä½¿ç”¨æŠ¢å å¼ Worker**
```bash
gcloud dataproc clusters create ticketmaster-cluster \
  --num-workers=2 \
  --num-preemptible-workers=2 \
  --preemptible-worker-boot-disk-size=50 \
  ...
```

### **3. åŠæ—¶åˆ é™¤é›†ç¾¤**
```bash
gcloud dataproc clusters delete ticketmaster-cluster \
  --region=us-east1 \
  --project=$PROJECT_ID
```

### **4. ä½¿ç”¨è‡ªåŠ¨ç¼©æ”¾**
```bash
gcloud dataproc clusters create ticketmaster-cluster \
  --enable-autoscaling \
  --autoscaling-policy=projects/$PROJECT_ID/regions/$REGION/autoscalingPolicies/default \
  ...
```

---

## ğŸ“Š ç›‘æ§ä½œä¸š

### **æŸ¥çœ‹ä½œä¸šçŠ¶æ€**
```bash
# åˆ—å‡ºæ‰€æœ‰ä½œä¸š
gcloud dataproc jobs list \
  --region=$REGION \
  --project=$PROJECT_ID

# æŸ¥çœ‹ç‰¹å®šä½œä¸š
gcloud dataproc jobs describe <JOB_ID> \
  --region=$REGION \
  --project=$PROJECT_ID
```

### **Web UI**
1. è®¿é—® GCP Console: https://console.cloud.google.com/dataproc
2. é€‰æ‹©ä½ çš„é›†ç¾¤
3. ç‚¹å‡» "Web Interfaces" æŸ¥çœ‹ Spark UI

---

## ğŸ”§ æ•…éšœæ’æŸ¥

### **é—®é¢˜ 1: æƒé™é”™è¯¯**
```bash
# ç¡®ä¿æœåŠ¡è´¦æˆ·æœ‰æƒé™è®¿é—® GCS
gcloud projects add-iam-policy-binding $PROJECT_ID \
  --member=serviceAccount:$SERVICE_ACCOUNT \
  --role=roles/storage.objectAdmin
```

### **é—®é¢˜ 2: é›†ç¾¤åˆ›å»ºå¤±è´¥**
```bash
# æ£€æŸ¥é…é¢
gcloud compute project-info describe --project=$PROJECT_ID

# æ£€æŸ¥ API æ˜¯å¦å¯ç”¨
gcloud services list --enabled --project=$PROJECT_ID
```

### **é—®é¢˜ 3: ä½œä¸šå¤±è´¥**
```bash
# æŸ¥çœ‹ä½œä¸šæ—¥å¿—
gcloud dataproc jobs describe <JOB_ID> \
  --region=$REGION \
  --project=$PROJECT_ID

# æŸ¥çœ‹ Spark æ—¥å¿—
gsutil cat gs://$BUCKET_NAME/google-cloud-dataproc-metainfo/<CLUSTER-UUID>/jobs/<JOB_ID>/driveroutput.000000000
```

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- **Dataproc æ–‡æ¡£:** https://cloud.google.com/dataproc/docs
- **å®šä»·è®¡ç®—å™¨:** https://cloud.google.com/products/calculator
- **æœ€ä½³å®è·µ:** https://cloud.google.com/dataproc/docs/concepts/iam/iam

---

## âœ… æ£€æŸ¥æ¸…å•

**è®¾ç½®å‰:**
- [ ] GCP è´¦æˆ·å·²åˆ›å»º
- [ ] gcloud CLI å·²å®‰è£…å¹¶è®¤è¯
- [ ] å·²å¯ç”¨å¿…è¦çš„ API
- [ ] å·²åˆ›å»º GCS Bucket

**è¿è¡Œå‰:**
- [ ] `dataproc_config.json` å·²æ­£ç¡®é…ç½®
- [ ] Dataproc é›†ç¾¤å·²åˆ›å»º
- [ ] æœ¬åœ°æ•°æ®å·²æ•´åˆ (`data/enriched_events.csv`)

**è¿è¡Œå:**
- [ ] æ£€æŸ¥ GCS è¾“å‡ºæ–‡ä»¶
- [ ] ä¸‹è½½ç»“æœåˆ°æœ¬åœ°
- [ ] åˆ é™¤ä¸´æ—¶é›†ç¾¤ï¼ˆèŠ‚çœæˆæœ¬ï¼‰

---

## ğŸ’¡ å¿«é€Ÿå‘½ä»¤å‚è€ƒ

```bash
# è®¾ç½®ç¯å¢ƒå˜é‡
export PROJECT_ID="your-project-id"
export BUCKET_NAME="your-bucket"
export REGION="us-east1"
export CLUSTER_NAME="ticketmaster-cluster"

# ä¸€é”®åˆ›å»ºé›†ç¾¤
gcloud dataproc clusters create $CLUSTER_NAME \
  --region=$REGION --num-workers=2 \
  --master-machine-type=n1-standard-4 \
  --worker-machine-type=n1-standard-4

# ä¸€é”®è¿è¡Œ
python quickstart_integration.py --mode dataproc

# ä¸€é”®æ¸…ç†
gcloud dataproc clusters delete $CLUSTER_NAME --region=$REGION
```

---

**éœ€è¦å¸®åŠ©ï¼Ÿ** æŸ¥çœ‹é¡¹ç›® README æˆ– EXTERNAL_DATA_WORKFLOW.md
