"""
å®Œæ•´çš„ master_df.csv æ•°æ®åˆ†ææµç¨‹

è¿™ä¸ªè„šæœ¬ä¼šä¾æ¬¡è¿è¡Œ:
1. ETL: æ•°æ®æ¸…æ´—å’Œè½¬æ¢
2. Analytics: å¤šç»´åº¦ç»Ÿè®¡åˆ†æ
3. ML: æœºå™¨å­¦ä¹ ä»·æ ¼é¢„æµ‹

ä½¿ç”¨æ–¹æ³•:
    æœ¬åœ°è¿è¡Œ: python run_master_pipeline.py --mode local
    Dataproc: python run_master_pipeline.py --mode dataproc
"""

import os
import sys
import subprocess
import argparse
import json


def load_config():
    """åŠ è½½ Dataproc é…ç½®"""
    if not os.path.exists("dataproc_config.json"):
        print("âŒ dataproc_config.json ä¸å­˜åœ¨")
        sys.exit(1)
    
    with open("dataproc_config.json", "r") as f:
        return json.load(f)


def run_command(cmd, description):
    """è¿è¡Œå‘½ä»¤å¹¶æ˜¾ç¤ºè¿›åº¦"""
    print(f"\n{'='*80}")
    print(f"ğŸš€ {description}")
    print(f"{'='*80}")
    print(f"å‘½ä»¤: {cmd}\n")
    
    result = subprocess.run(cmd, shell=True)
    
    if result.returncode == 0:
        print(f"\nâœ… {description} - å®Œæˆ")
        return True
    else:
        print(f"\nâŒ {description} - å¤±è´¥ (é€€å‡ºç : {result.returncode})")
        return False


def run_local():
    """æœ¬åœ°è¿è¡Œæ¨¡å¼"""
    print("\n" + "="*80)
    print("ğŸ“ æœ¬åœ°æ¨¡å¼ - ä½¿ç”¨ master_df.csv")
    print("="*80)
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶
    if not os.path.exists("data/master_df.csv"):
        print("âŒ data/master_df.csv ä¸å­˜åœ¨")
        return False
    
    print("âœ… æ•°æ®æ–‡ä»¶å­˜åœ¨")
    
    # Step 1: ETL
    if not run_command(
        "spark-submit spark_etl_master.py --input data/master_df.csv --output output/master_parquet",
        "æ­¥éª¤ 1/3: ETL - æ•°æ®æ¸…æ´—ä¸è½¬æ¢"
    ):
        return False
    
    # Step 2: Analytics
    if not run_command(
        "spark-submit spark_analysis_master.py --input output/master_parquet --output output/analytics",
        "æ­¥éª¤ 2/3: åˆ†æ - å¤šç»´åº¦ç»Ÿè®¡"
    ):
        return False
    
    # Step 3: å•æ¨¡å‹ ML(MAX)
    if not run_command(
        "spark-submit spark_ml_master_max.py --input output/master_parquet --output output/ml_results_max --model-type rf",
        "æ­¥éª¤ 3/4: æœºå™¨å­¦ä¹  - å•æ¨¡å‹ä»·æ ¼é¢„æµ‹ (RandomForest)(MAX)"
    ):
        return False
    
    # Step 4: å¤šæ¨¡å‹å¯¹æ¯”è®­ç»ƒ(MAX)
    if not run_command(
        "spark-submit spark_ml_multi_models_max.py --input output/master_parquet --output output/ml_multi_models_max",
        "æ­¥éª¤ 4/4: æœºå™¨å­¦ä¹  - å¤šæ¨¡å‹å¯¹æ¯”è®­ç»ƒ (6ç§æ¨¡å‹)(MAX)"
    ):
        return False
    
    # Step 5: å•æ¨¡å‹ ML(MIN)
    if not run_command(
        "spark-submit spark_ml_master_min.py --input output/master_parquet --output output/ml_results_min --model-type rf",
        "æ­¥éª¤ 3/4: æœºå™¨å­¦ä¹  - å•æ¨¡å‹ä»·æ ¼é¢„æµ‹ (RandomForest)(MIN)"
    ):
        return False
    
    # Step 6: å¤šæ¨¡å‹å¯¹æ¯”è®­ç»ƒ(MIN)
    if not run_command(
        "spark-submit spark_ml_multi_models_min.py --input output/master_parquet --output output/ml_multi_models_min",
        "æ­¥éª¤ 4/4: æœºå™¨å­¦ä¹  - å¤šæ¨¡å‹å¯¹æ¯”è®­ç»ƒ (6ç§æ¨¡å‹)(MIN)"
    ):
        return False
    
    print("\n" + "="*80)
    print("âœ… æœ¬åœ°æµç¨‹å®Œæˆï¼")
    print("="*80)
    print("\nğŸ“ ç»“æœä½ç½®:")
    print("  - ETL è¾“å‡º: output/master_parquet/")
    print("  - åˆ†æç»“æœ: output/analytics/")
    print("  - å•æ¨¡å‹ ML(MAX): output/ml_results_max/")
    print("  - å¤šæ¨¡å‹å¯¹æ¯”(MAX): output/ml_multi_models_max/")
    print("  - å•æ¨¡å‹ ML(MIN): output/ml_results_min/")
    print("  - å¤šæ¨¡å‹å¯¹æ¯”(MIN): output/ml_multi_models_min/")
    
    return True


def run_dataproc():
    """Dataproc è¿è¡Œæ¨¡å¼"""
    print("\n" + "="*80)
    print("â˜ï¸  Dataproc æ¨¡å¼ - ä½¿ç”¨ master_df.csv")
    print("="*80)
    
    config = load_config()
    project = config['project_id']
    region = config['region']
    cluster = config['cluster_name']
    bucket = config['bucket_name']
    
    print(f"\né…ç½®ä¿¡æ¯:")
    print(f"  é¡¹ç›®: {project}")
    print(f"  åŒºåŸŸ: {region}")
    print(f"  é›†ç¾¤: {cluster}")
    print(f"  å­˜å‚¨æ¡¶: {bucket}")
    
    # Step 1: ä¸Šä¼ æ•°æ®å’Œè„šæœ¬
    print("\nğŸ“¤ ä¸Šä¼ æ–‡ä»¶åˆ° GCS...")
    
    uploads = [
        ("data/master_df.csv", f"gs://{bucket}/data/master_df.csv"),
        ("spark_etl_master.py", f"gs://{bucket}/scripts/spark_etl_master.py"),
        ("spark_analysis_master.py", f"gs://{bucket}/scripts/spark_analysis_master.py"),
        ("spark_ml_master_max.py", f"gs://{bucket}/scripts/spark_ml_master_max.py"),
        ("spark_ml_multi_models_max.py", f"gs://{bucket}/scripts/spark_ml_multi_models_max.py"),
        ("spark_ml_master_min.py", f"gs://{bucket}/scripts/spark_ml_master_min.py"),
        ("spark_ml_multi_models_min.py", f"gs://{bucket}/scripts/spark_ml_multi_models_min.py")
    ]
    
    for local_file, gcs_path in uploads:
        if not os.path.exists(local_file):
            print(f"âŒ {local_file} ä¸å­˜åœ¨")
            return False
        
        cmd = f"gsutil cp {local_file} {gcs_path}"
        if not run_command(cmd, f"ä¸Šä¼  {local_file}"):
            return False
    
    # Step 2: æäº¤ ETL ä½œä¸š
    etl_cmd = f"""gcloud dataproc jobs submit pyspark gs://{bucket}/scripts/spark_etl_master.py \
        --cluster={cluster} \
        --region={region} \
        --project={project} \
        -- --input gs://{bucket}/data/master_df.csv \
           --output gs://{bucket}/output/master_parquet"""
    
    if not run_command(etl_cmd, "æ­¥éª¤ 1/4: Dataproc ETL ä½œä¸š"):
        return False
    
    # Step 3: æäº¤åˆ†æä½œä¸š
    analysis_cmd = f"""gcloud dataproc jobs submit pyspark gs://{bucket}/scripts/spark_analysis_master.py \
        --cluster={cluster} \
        --region={region} \
        --project={project} \
        -- --input gs://{bucket}/output/master_parquet \
           --output gs://{bucket}/output/analytics"""
    
    if not run_command(analysis_cmd, "æ­¥éª¤ 2/4: Dataproc åˆ†æä½œä¸š"):
        return False
    
    # Step 4: æäº¤å•æ¨¡å‹ ML ä½œä¸š(MAX)
    ml_cmd = f"""gcloud dataproc jobs submit pyspark gs://{bucket}/scripts/spark_ml_master_max.py \
        --cluster={cluster} \
        --region={region} \
        --project={project} \
        -- --input gs://{bucket}/output/master_parquet \
           --output gs://{bucket}/output/ml_results_max \
           --model-type rf"""
    
    if not run_command(ml_cmd, "æ­¥éª¤ 3/4: Dataproc å•æ¨¡å‹ ML ä½œä¸š(MAX)"):
        return False
    
    # Step 5: æäº¤å¤šæ¨¡å‹å¯¹æ¯”è®­ç»ƒä½œä¸š(MAX)
    multi_ml_cmd = f"""gcloud dataproc jobs submit pyspark gs://{bucket}/scripts/spark_ml_multi_models_max.py \
        --cluster={cluster} \
        --region={region} \
        --project={project} \
        -- --input gs://{bucket}/output/master_parquet \
           --output gs://{bucket}/output/ml_multi_models_max"""
    
    if not run_command(multi_ml_cmd, "æ­¥éª¤ 4/4: Dataproc å¤šæ¨¡å‹å¯¹æ¯”è®­ç»ƒ(MAX)"):
        return False

    # Step 6: æäº¤å•æ¨¡å‹ ML ä½œä¸š(MIN)
    ml_cmd = f"""gcloud dataproc jobs submit pyspark gs://{bucket}/scripts/spark_ml_master_min.py \
        --cluster={cluster} \
        --region={region} \
        --project={project} \
        -- --input gs://{bucket}/output/master_parquet \
           --output gs://{bucket}/output/ml_results_min \
           --model-type rf"""
    
    if not run_command(ml_cmd, "æ­¥éª¤ 3/4: Dataproc å•æ¨¡å‹ ML ä½œä¸š(MIN)"):
        return False
    
    # Step 7: æäº¤å¤šæ¨¡å‹å¯¹æ¯”è®­ç»ƒä½œä¸š(MIN)
    multi_ml_cmd = f"""gcloud dataproc jobs submit pyspark gs://{bucket}/scripts/spark_ml_multi_models_min.py \
        --cluster={cluster} \
        --region={region} \
        --project={project} \
        -- --input gs://{bucket}/output/master_parquet \
           --output gs://{bucket}/output/ml_multi_models_min"""
    
    if not run_command(multi_ml_cmd, "æ­¥éª¤ 4/4: Dataproc å¤šæ¨¡å‹å¯¹æ¯”è®­ç»ƒ(MIN)"):
        return False
    
    print("\n" + "="*80)
    print("âœ… Dataproc æµç¨‹å®Œæˆï¼")
    print("="*80)
    print(f"\nğŸ“ GCS ç»“æœä½ç½®:")
    print(f"  - ETL è¾“å‡º: gs://{bucket}/output/master_parquet/")
    print(f"  - åˆ†æç»“æœ: gs://{bucket}/output/analytics/")
    print(f"  - å•æ¨¡å‹ ML(MAX): gs://{bucket}/output/ml_results_max/")
    print(f"  - å¤šæ¨¡å‹å¯¹æ¯”(MAX): gs://{bucket}/output/ml_multi_models_max/")
    print(f"  - å•æ¨¡å‹ ML(MIN): gs://{bucket}/output/ml_results_min/")
    print(f"  - å¤šæ¨¡å‹å¯¹æ¯”(MIN): gs://{bucket}/output/ml_multi_models_min/")
    print(f"\nğŸ’¡ ä¸‹è½½ç»“æœ:")
    print(f"  gsutil -m cp -r gs://{bucket}/output/ ./")
    
    return True


def main():
    parser = argparse.ArgumentParser(description="è¿è¡Œ master_df.csv å®Œæ•´åˆ†ææµç¨‹")
    parser.add_argument(
        "--mode",
        type=str,
        required=True,
        choices=["local", "dataproc"],
        help="è¿è¡Œæ¨¡å¼: local (æœ¬åœ°) æˆ– dataproc (äº‘ç«¯)"
    )
    
    args = parser.parse_args()
    
    if args.mode == "local":
        success = run_local()
    else:
        success = run_dataproc()
    
    if success:
        print("\nğŸ‰ æ‰€æœ‰æ­¥éª¤æˆåŠŸå®Œæˆï¼")
        sys.exit(0)
    else:
        print("\nâŒ æµç¨‹æ‰§è¡Œå¤±è´¥")
        sys.exit(1)


if __name__ == "__main__":
    main()
