#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Spark å¤šæ¨¡å‹è®­ç»ƒ - master_df.csv æ•°æ®

è®­ç»ƒ 6 ç§å›å½’æ¨¡å‹é¢„æµ‹äºŒçº§å¸‚åœºä»·æ ¼ï¼ˆSeatGeek å¹³å‡ä»·æ ¼ï¼‰:
  1. Linear Regression (æ™®é€šçº¿æ€§å›å½’)
  2. Lasso Regression (L1 æ­£åˆ™åŒ–)
  3. Elastic Net (L1+L2 æ­£åˆ™åŒ–)
  4. Decision Tree (å†³ç­–æ ‘)
  5. Random Forest (éšæœºæ£®æ—)
  6. Gradient Boosting Tree (æ¢¯åº¦æå‡æ ‘)

Usage:
æœ¬åœ°è¿è¡Œ:
    spark-submit spark_ml_multi_models.py \
      --input output/master_parquet \
      --output output/ml_multi_models

Dataproc:
    gcloud dataproc jobs submit pyspark spark_ml_multi_models.py \
      --cluster=<cluster-name> \
      --region=us-east1 \
      -- --input gs://bucket/output/master_parquet \
         --output gs://bucket/output/ml_multi_models
"""

import argparse
from pyspark.sql import SparkSession
from pyspark.sql import functions as F

from pyspark.ml import Pipeline
from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler
from pyspark.ml.regression import LinearRegression, DecisionTreeRegressor, RandomForestRegressor, GBTRegressor
from pyspark.ml.evaluation import RegressionEvaluator


def build_spark(app_name: str = "MultiModelML") -> SparkSession:
    spark = (
        SparkSession.builder
        .appName(app_name)
        .config("spark.sql.adaptive.enabled", "true")
        .getOrCreate()
    )
    return spark


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument(  
        "--input",
        type=str,
        required=True,
        help="Input Parquet path from ETL step"
    )
    parser.add_argument(
        "--output",
        type=str,
        required=True,
        help="Output base path for models and metrics"
    )
    return parser.parse_args()


def main():
    args = parse_args()
    spark = build_spark()

    print(f"[INFO] Reading processed data from: {args.input}")
    df = spark.read.parquet(args.input)
    print("[INFO] Schema:")
    df.printSchema()
    
    # æ•°æ®å‡†å¤‡
    print("[INFO] Preparing data for ML...")
    df_ml = (
        df.filter(
            F.col("tm_max_price").isNotNull() &
            (F.col("tm_max_price") > 0) &
            F.col("artist").isNotNull()
        )
        # å¡«å……ç¼ºå¤±å€¼
        .fillna({
            "spotify_popularity": 0,
            "genre": "Unknown",
            "subgenre": "Unknown",
            "city": "Unknown",
            "state": "Unknown"
        })
    )
    print(f"[INFO] ML dataset size: {df_ml.count()}")
    
    # åˆ†å‰²æ•°æ®é›†
    print("[INFO] Splitting data: 80% train, 20% test...")
    train_df, test_df = df_ml.randomSplit([0.8, 0.2], seed=42)
    print(f"  Train set: {train_df.count()}")
    print(f"  Test set: {test_df.count()}")

    # # ç”¨å¹³å‡ä»·æ ¼ä»£æ›¿ç±»åˆ«æ ‡ç­¾ï¼Œé¿å…ç±»åˆ«æ ‡ç­¾ç»´åº¦è¿‡é«˜å½±å“å¤§
    # def add_avg_encoding(df, ref_df, col):
    #     avg_df = (
    #         ref_df
    #         .groupBy(col)
    #         .agg(F.avg("tm_max_price").alias(f"{col}_avg_price"))
    #     )
    #     return df.join(avg_df, on=col, how="left")

    global_avg = train_df.select(F.avg("tm_max_price")).first()[0]  

    def add_avg_encoding(df, ref_df, col):
        avg_df = (
            ref_df
            .groupBy(col)
            .agg(F.avg("tm_max_price").alias(f"{col}_avg_price"))
        )
        return df.join(avg_df, on=col, how="left") \
                .fillna({f"{col}_avg_price": global_avg})        
    
    # åªç”¨train dataå–å¹³å‡
    for c in ["state", "city", "genre", "subgenre"]:
        train_df = add_avg_encoding(train_df, train_df, c)
        test_df  = add_avg_encoding(test_df,  train_df, c)
        
    # è¡¥ä¸Šnull
    avg_cols = ["state_avg_price", "city_avg_price", "genre_avg_price", "subgenre_avg_price"]
    train_df = train_df.fillna(0, subset=avg_cols)
    test_df  = test_df.fillna(0, subset=avg_cols)
    
    # ç‰¹å¾å·¥ç¨‹
    print("[INFO] Feature engineering...")
    
    # æ•°å€¼ç‰¹å¾
    numeric_features = ["spotify_popularity", "year", "month", "weekday", 
                        "state_avg_price", "city_avg_price", "genre_avg_price", "subgenre_avg_price"]

    # ç»„åˆæ‰€æœ‰ç‰¹å¾
    assembler = VectorAssembler(
        inputCols=numeric_features,
        outputCol="features",
        handleInvalid="error"
    )
    
    # æ„å»ºå¹¶åº”ç”¨ç‰¹å¾å·¥ç¨‹ Pipeline
    print("[INFO] Applying feature engineering...")
    pipeline = Pipeline(stages=[assembler])
    feature_model = pipeline.fit(train_df)
    train_data = feature_model.transform(train_df)
    test_data = feature_model.transform(test_df)

    # ç¼“å­˜æ•°æ®ä»¥åŠ é€Ÿå¤šæ¬¡è®­ç»ƒ
    train_data = train_data.cache()
    test_data = test_data.cache()

    print(f"[INFO] Feature vector size: {train_data.select('features').head()[0].size}")

    # ============================================================
    # å®šä¹‰ 6 ç§å›å½’æ¨¡å‹
    # ============================================================
    print("\n" + "="*80)
    print("å®šä¹‰å¤šä¸ªå›å½’æ¨¡å‹")
    print("="*80)

    # 1. æ™®é€šçº¿æ€§å›å½’
    lr = LinearRegression(
        featuresCol="features",
        labelCol="tm_max_price",
        maxIter=100
    )

    # 2. Lasso å›å½’ï¼ˆL1 æ­£åˆ™åŒ–ï¼‰
    lasso = LinearRegression(
        featuresCol="features",
        labelCol="tm_max_price",
        regParam=0.1,
        elasticNetParam=1.0,  # 1.0 = çº¯ L1
        maxIter=100
    )

    # 3. Elastic Netï¼ˆL1 + L2 æ­£åˆ™åŒ–ï¼‰
    elastic_net = LinearRegression(
        featuresCol="features",
        labelCol="tm_max_price",
        regParam=0.1,
        elasticNetParam=0.7,  # 0.5 = L1 å’Œ L2 æ··åˆ
        maxIter=100
    )

    # 4. å†³ç­–æ ‘
    dt = DecisionTreeRegressor(
        featuresCol="features",
        labelCol="tm_max_price",
        maxDepth=8,
        seed=42
    )

    # 5. éšæœºæ£®æ—
    rf = RandomForestRegressor(
        featuresCol="features",
        labelCol="tm_max_price",
        numTrees=100,
        maxDepth=20,
        seed=42
    )

    # 6. æ¢¯åº¦æå‡æ ‘
    gbt = GBTRegressor(
        featuresCol="features",
        labelCol="tm_max_price",
        maxIter=50,
        maxDepth=2,
        seed=42
    )

    regression_models = {
        "linear_regression": lr,
        "lasso_regression": lasso,
        "elastic_net": elastic_net,
        "decision_tree": dt,
        "random_forest": rf,
        "gbt": gbt
    }

    print(f"[INFO] å°†è®­ç»ƒ {len(regression_models)} ä¸ªå›å½’æ¨¡å‹")

    # ============================================================
    # è®­ç»ƒå’Œè¯„ä¼°æ‰€æœ‰æ¨¡å‹
    # ============================================================
    evaluator_rmse = RegressionEvaluator(metricName="rmse", labelCol="tm_max_price", predictionCol="prediction")
    evaluator_mae = RegressionEvaluator(metricName="mae", labelCol="tm_max_price", predictionCol="prediction")
    evaluator_r2 = RegressionEvaluator(metricName="r2", labelCol="tm_max_price", predictionCol="prediction")

    metrics_list = []
    
    for model_name, model in regression_models.items():
        print("\n" + "="*80)
        print(f"è®­ç»ƒæ¨¡å‹: {model_name}")
        print("="*80)

        # è®­ç»ƒæ¨¡å‹
        trained_model = model.fit(train_data)
        
        # é¢„æµ‹
        pred_df = trained_model.transform(test_data)

        # è¯„ä¼°
        rmse = evaluator_rmse.evaluate(pred_df)
        mae = evaluator_mae.evaluate(pred_df)
        r2 = evaluator_r2.evaluate(pred_df)

        print(f"[METRICS]")
        print(f"  RMSE (Root Mean Squared Error): ${rmse:.2f}")
        print(f"  MAE  (Mean Absolute Error):     ${mae:.2f}")
        print(f"  RÂ²   (R-squared):                {r2:.4f}")

        # ä¿å­˜æ¨¡å‹
        model_path = f"{args.output}/models/{model_name}"
        print(f"[INFO] ä¿å­˜æ¨¡å‹åˆ°: {model_path}")
        trained_model.write().overwrite().save(model_path)

        # ä¿å­˜é¢„æµ‹æ ·ä¾‹
        predictions_sample = pred_df.select(
            "event_id",
            "artist",
            "genre",
            F.col("tm_max_price").alias("actual_price"),
            F.col("prediction").alias("predicted_price"),
            F.abs(F.col("tm_max_price") - F.col("prediction")).alias("error")
        ).limit(100)
        
        sample_output = f"{args.output}/predictions_sample/{model_name}"
        print(f"[INFO] ä¿å­˜é¢„æµ‹æ ·ä¾‹åˆ°: {sample_output}")
        predictions_sample.coalesce(1).write.mode("overwrite").option("header", "true").csv(sample_output)

        # è®°å½•æŒ‡æ ‡
        metrics_list.append((model_name, float(rmse), float(mae), float(r2)))

    # ============================================================
    # ä¿å­˜æ‰€æœ‰æ¨¡å‹çš„å¯¹æ¯”æŒ‡æ ‡
    # ============================================================
    metrics_output = f"{args.output}/metrics_comparison"
    print(f"\n[INFO] ä¿å­˜æ‰€æœ‰æ¨¡å‹å¯¹æ¯”æŒ‡æ ‡åˆ°: {metrics_output}")
    
    metrics_schema = ["model", "rmse", "mae", "r2"]
    metrics_df = spark.createDataFrame(metrics_list, metrics_schema)
    
    # ä¿å­˜ä¸º CSV
    metrics_df.coalesce(1).write.mode("overwrite").option("header", "true").csv(metrics_output + "_csv")
    
    # ä¿å­˜ä¸º JSON
    metrics_df.coalesce(1).write.mode("overwrite").json(metrics_output + "_json")

    # æ˜¾ç¤ºå¯¹æ¯”ç»“æœ
    print("\n" + "="*80)
    print("æ‰€æœ‰æ¨¡å‹æ€§èƒ½å¯¹æ¯”")
    print("="*80)
    metrics_df.orderBy("rmse").show(truncate=False)

    # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
    best_model = metrics_df.orderBy("rmse").first()
    print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model['model']}")
    print(f"   RMSE: ${best_model['rmse']:.2f}")
    print(f"   MAE:  ${best_model['mae']:.2f}")
    print(f"   RÂ²:   {best_model['r2']:.4f}")

    print("\n[INFO] å¤šæ¨¡å‹è®­ç»ƒå®Œæˆï¼")
    print("\nğŸ“ è¾“å‡ºä½ç½®:")
    print(f"  - æ¨¡å‹æ–‡ä»¶:     {args.output}/models/")
    print(f"  - é¢„æµ‹æ ·ä¾‹:     {args.output}/predictions_sample/")
    print(f"  - æŒ‡æ ‡å¯¹æ¯”:     {args.output}/metrics_comparison_csv/")
    
    spark.stop()


if __name__ == "__main__":
    main()
